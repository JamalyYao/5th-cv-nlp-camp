{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/sample_train.txt\", delimiter=\"\\t\")\n",
    "test = pd.read_csv(\"../data/test_id.txt\", delimiter=\"\\t\")\n",
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练好的模型结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cook_book = {\n",
    "    \"lgb_572\":5,\n",
    "    \"lgb_628\":5,\n",
    "    \"lgb_585\":5,\n",
    "    \"lgb_692\":5,\n",
    "    \"xgb_572\":5,\n",
    "#     \"xgb_652\":1,\n",
    "    \"xgb_652_random\":1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, count in cook_book.items():\n",
    "    model_dir = \"../model_output/178/%s/\" % name\n",
    "    for i in range(count):\n",
    "        pred_train = pd.read_csv(model_dir + str(i) + \"/pred_train.csv\")\n",
    "        pred_test = pd.read_csv(model_dir + str(i) + \"/all_test_preds.csv\")\n",
    "\n",
    "        pred_train['rank_prob'] = pred_train.prob.rank() / train.shape[0]\n",
    "        pred_test[\"rank_prob\"] = pred_test.groupby(\"fold_id\").prob.rank() / test.shape[0]\n",
    "        pred_test = pred_test.groupby(\"id\").mean().reset_index().drop(\"fold_id\", axis=1)\n",
    "\n",
    "        a = pred_test.prob.rank()\n",
    "        b = pred_test.rank_prob.rank()\n",
    "\n",
    "        d = pd.concat([pred_train, pred_test], axis=0)\n",
    "        d = d.rename(columns={\"prob\": \"%s_%d_prob\" % (name, i), \"rank_prob\": \"%s_%d_rank_prob\" % (name, i)})\n",
    "\n",
    "        df = df.merge(d, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  process_feature(train_x, valid_x, test_df):\n",
    "    result = []\n",
    "    drop_cols = ['id','label']\n",
    "    for df in [train_x, valid_x, test_df]:\n",
    "        result.append(df.drop(drop_cols, axis=1))\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(df, num_folds, param, model_dir, classfier=\"lr\", stratified=True, debug=False):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    train_df = df[df.label.notnull()]\n",
    "    test_df = df[df.label.isnull()]\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=178)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=178)\n",
    "\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    all_test_preds = []    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "#     feats = [f for f in train_df.columns if f not in ['id','label', \"prob\", \"tag\", \"loan_dt\"]]\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, train_df['label'])):\n",
    "        train_x, train_y = train_df.iloc[train_idx], train_df['label'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df.iloc[valid_idx], train_df['label'].iloc[valid_idx]\n",
    "        fold_preds = test_df[[\"id\"]]\n",
    "        \n",
    "        train_x, valid_x, test = process_feature(train_x, valid_x, test_df)\n",
    "        if n_fold == 0:\n",
    "            print(train_x.shape, valid_x.shape, test.shape)\n",
    "        \n",
    "        if classfier == \"lgb\":\n",
    "            train_data = lgb.Dataset(train_x, label=train_y)\n",
    "            validation_data = lgb.Dataset(valid_x, label=valid_y)\n",
    "\n",
    "            clf=lgb.train(params,\n",
    "                          train_data,\n",
    "                          num_boost_round=10000,\n",
    "                          valid_sets=[train_data, validation_data],\n",
    "                          valid_names=[\"train\", \"valid\"],\n",
    "                          early_stopping_rounds=200,\n",
    "                          verbose_eval=100)\n",
    "\n",
    "            valid_preds = clf.predict(valid_x, num_iteration=clf.best_iteration)\n",
    "            test_preds = clf.predict(test, num_iteration=clf.best_iteration)\n",
    "        \n",
    "        if classfier == \"lr\":\n",
    "            clf = LogisticRegression(penalty=\"l2\", solver=\"sag\", n_jobs=32)\n",
    "            clf.fit(train_x, train_y)\n",
    "\n",
    "            valid_preds = clf.predict_proba(valid_x)[:, 1]\n",
    "            test_preds = clf.predict_proba(test)[:, 1]\n",
    "\n",
    "        fold_preds['prob'] = test_preds\n",
    "        fold_preds['fold_id'] = n_fold + 1\n",
    "        all_test_preds.append(fold_preds)\n",
    "\n",
    "        oof_preds[valid_idx] = valid_preds\n",
    "        \n",
    "        if classfier == \"lgb\":\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = train_x.columns.tolist()\n",
    "            fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, valid_preds)))\n",
    "        \n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['label'], oof_preds))\n",
    "    \n",
    "    if not debug:\n",
    "        train_df[\"prob\"] = oof_preds\n",
    "        train_df[['id', 'prob']].to_csv(model_dir + \"pred_train.csv\", index= False)\n",
    "\n",
    "        all_test_preds = pd.concat(all_test_preds, axis=0)\n",
    "        all_test_preds.to_csv(model_dir + \"all_test_preds.csv\", index=False)\n",
    "        \n",
    "        sub = pd.DataFrame()\n",
    "        sub['id'] = all_test_preds.id.unique()\n",
    "        sub.set_index(\"id\", inplace=True)\n",
    "        sub[\"prob\"] = all_test_preds.groupby(\"id\").prob.mean()\n",
    "        sub.reset_index().to_csv(model_dir + \"sub_test.txt\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 27 features.\n"
     ]
    }
   ],
   "source": [
    "X = df[df.label.notna()].drop(['id', 'label'], axis=1)\n",
    "y = df[df.label.notna()].label\n",
    "feature_name = X.columns.tolist()\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "feature_score = pd.DataFrame(index=X.columns.tolist())\n",
    "# pearson_cor\n",
    "def pearson_cor(X, y):\n",
    "    cor_list = []\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    return np.abs(cor_list)\n",
    "feature_score[\"pearson_cor\"] = pearson_cor(X, y)\n",
    "feature_score[\"pearson_cor_rank\"] = feature_score.pearson_cor.rank(ascending=False)\n",
    "\n",
    "# chi2\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "chi_selector = SelectKBest(chi2)\n",
    "chi_selector.fit(X_norm, y)\n",
    "chi_score = chi_selector.scores_\n",
    "chi_feature = X.iloc[:,np.argsort(chi_score)[::-1]].columns.tolist()\n",
    "feature_score[\"chi_2\"] = chi_score\n",
    "feature_score[\"chi_2\"] = feature_score[\"chi_2\"].fillna(0)\n",
    "feature_score[\"chi_2_rank\"] = feature_score.chi_2.rank(ascending=False)\n",
    "\n",
    "# lgb\n",
    "lgb = LGBMClassifier(\n",
    "            nthread=20,\n",
    "            #is_unbalance=True,\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=8,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.04,\n",
    "            reg_lambda=0.073,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=40,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "            #scale_pos_weight=11\n",
    "            )\n",
    "rfe_selector = RFE(lgb, step=0.1, verbose=1)\n",
    "rfe_selector.fit(X, y)\n",
    "feature_score[\"ref_rank\"] = rfe_selector.ranking_\n",
    "rank_count = feature_score.ref_rank.value_counts().sort_index().tolist()\n",
    "feature_score[\"ref_rank\"] = feature_score[\"ref_rank\"].apply(lambda x: sum(rank_count[:x]))\n",
    "\n",
    "# lr\n",
    "lr = LogisticRegression(penalty=\"l2\", solver=\"sag\", n_jobs=20)\n",
    "lr.fit(X_norm, y)\n",
    "feature_score[\"lr\"] = np.abs(lr.coef_)[0]\n",
    "feature_score[\"lr_rank\"] = feature_score.lr.rank(ascending=False)\n",
    "\n",
    "# rf\n",
    "rf = RandomForestClassifier(n_jobs=20)\n",
    "rf.fit(X, y)\n",
    "feature_score[\"rf\"] = rf.feature_importances_\n",
    "feature_score[\"rf_rank\"] = feature_score.rf.rank(ascending=False)\n",
    "\n",
    "# total rank\n",
    "feature_score[\"rank_\"] = feature_score.chi_2_rank + feature_score.pearson_cor_rank + feature_score.ref_rank + feature_score.rf_rank + feature_score.lr_rank\n",
    "\n",
    "# feature_score.sort_values(\"rank_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = feature_score.sort_values(\"rank_\").head(9).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_652_random_0_prob',\n",
       " 'xgb_652_random_0_rank_prob',\n",
       " 'lgb_628_0_rank_prob',\n",
       " 'lgb_572_4_prob',\n",
       " 'xgb_572_0_prob',\n",
       " 'xgb_572_1_prob',\n",
       " 'lgb_572_2_prob',\n",
       " 'lgb_628_1_rank_prob',\n",
       " 'lgb_585_2_rank_prob']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'boosting_type': 'goss', \n",
    "          'colsample_bytree': 0.6188451188326409, \n",
    "          'learning_rate': 0.02278643912197006,\n",
    "          'max_bin': 200, \n",
    "          'metric': 'auc', \n",
    "          'min_child_weight': 19.91986754624656,\n",
    "          'num_leaves': 71, \n",
    "          'reg_alpha': 3.6554523524605216, \n",
    "          'reg_lambda': 2676.0505164555602, \n",
    "          'subsample': 1.0}\n",
    "model_dir = \"../model_output/178/stacking/\"\n",
    "result = cv(df[[\"id\", \"label\"] + fs[1:]], 5, params, model_dir, \"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
